---
permalink: /
title: "Yuanzhi Zhu - Homepage"
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# ðŸ‘¦ About Me

I am a researcher specializing in Vision-Language Models (VLM) and Large Language Models (LLM), currently working as a Senior Algorithm Engineer at Alibaba Tongyi Lab (formerly Alibaba DAMO Academy), also as a core team member of Qwen2.5-VL, Qwen2.5-Omni, Qwen3, etc. I received both my bachelorâ€™s and masterâ€™s degrees from South China University of Technology, where I focused on Computer Vision (CV) and Optical Character Recognition (OCR) research. After graduation, I joined Alibaba, where I have conducted research in OCR, text-to-image generation (T2I), VLMs, and LLMs. My work has contributed to the advancement of OCR expert models and the large model era, with multiple technological breakthroughs achieved. Combining strong hands-on implementation skills with deep algorithm development expertise, I have co-authored over ten publications in top-tier conferences and journals such as CVPR, ECCV, and AAAI, including four as the first author. My work has accumulated over 3,000 citations on Google Scholar. 


# ðŸ“š Selected Publications

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/Qwen2.5-VL-framework.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923)

Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, <span style="color:PaleVioletRed;">Yuanzhi Zhu</span>, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin.

[PDF](https://arxiv.org/abs/2502.13923) [Code](https://github.com/QwenLM/Qwen2.5-VL/) [Slides](https://www.bilibili.com/video/BV1fXRwY2EtX) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- One of the Worldâ€™s Most Advanced Vision-Language (VL) Large Models. 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/SceneVTG-poster.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Visual Text Generation in the Wild](https://arxiv.org/abs/2501.02962)

<span style="color:PaleVioletRed;">Yuanzhi Zhu</span>, Jiawei Liu, Feiyu Gao, Wenyu Liu, Xinggang Wang, Peng Wang, Fei Huang, Cong Yao, Zhibo Yang.

[PDF](https://arxiv.org/abs/2501.02962) [Code](https://github.com/AlibabaResearch/AdvancedLiterateMachinery) [Slides](https://www.youtube.com/watch?v=fZXotgUxm8w) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- This work focuses on generating realistic visual text images in real-world scenes by leveraging LLMs and conditional diffusion models. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/CTIG-poster.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Conditional Text Image Generation with Diffusion Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Conditional_Text_Image_Generation_With_Diffusion_Models_CVPR_2023_paper.pdf)

<span style="color:PaleVioletRed;">Yuanzhi Zhu</span>, Zhaohai Li, Tianwei Wang, Mengchao He, Cong Yao.

[PDF](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Conditional_Text_Image_Generation_With_Diffusion_Models_CVPR_2023_paper.pdf) [Slides](https://www.youtube.com/watch?v=G09Hf7on4oc) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- The first work to propose controllable text-line image generation for multi-scenario applications. 
</div>
</div>


# ðŸ”¬ More Papers

[//]: # (- Qwen2. 5-vl technical report. 2025. [[pdf]](https://arxiv.org/abs/2502.13923) Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, <span style="color:PaleVioletRed;">Yuanzhi Zhu</span>, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin.)
[//]: # (- Visual text generation in the wild. ECCV 2024. [[pdf]](https://arxiv.org/abs/2501.02962) <span style="color:PaleVioletRed;">Yuanzhi Zhu</span>, Jiawei Liu, Feiyu Gao, Wenyu Liu, Xinggang Wang, Peng Wang, Fei Huang, Cong Yao, Zhibo Yang.)
[//]: # (- Conditional text image generation with diffusion models. CVPR 2023 [[pdf]](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Conditional_Text_Image_Generation_With_Diffusion_Models_CVPR_2023_paper.pdf) <span style="color:PaleVioletRed;">Yuanzhi Zhu</span>, Zhaohai Li, Tianwei Wang, Mengchao He, Cong Yao.)
- Qwen3 Technical Report. **arXiv 2025**. [[pdf]](https://arxiv.org/abs/2505.09388) An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui et al. [Contributors]
- Qwen2.5-Omni Technical Report. **arXiv 2025**. [[pdf]](https://arxiv.org/abs/2503.20215) Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai et al. [Contributors]
- Implicit Feature Alignment: Learn to Convert Text Recognizer to Text Spotter. **CVPR 2021**. [[pdf]](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Implicit_Feature_Alignment_Learn_To_Convert_Text_Recognizer_to_Text_CVPR_2021_paper.pdf) Tianwei$^{\*}$ Wang, <span style="color:PaleVioletRed;">Yuanzhi Zhu$^{\*}$</span>, Lianwen Jin, Dezhi Peng, Zhe Li, Mengchao He, Yongpan Wang, Canjie Luo.
- Hiercode: A Lightweight Hierarchical Codebook for Zero-Shot Chinese Text Recognition. **PR 2025**. [[pdf]](https://www.sciencedirect.com/science/article/abs/pii/S0031320324007143) Yuyi Zhang$^{\*}$, <span style="color:PaleVioletRed;">Yuanzhi Zhu$^{\*}$</span>, Dezhi Peng, Peirong Zhang, Zhenhua Yang, Zhibo Yang, Cong Yao, Lianwen Jin.
- SceneVTG++: Controllable Multilingual Visual Text Generation in the Wild. **arXiv 2025**. [[pdf]](https://arxiv.org/pdf/2501.02962) Jiawei Liu, <span style="color:PaleVioletRed;">Yuanzhi Zhu</span>, Feiyu Gao, Zhibo Yang, Peng Wang, Junyang Lin, Xinggang Wang, Wenyu Liu.
- SLOGAN: Handwriting Style Synthesis for Arbitrary-Length and Out-of-Vocabulary Text. **IEEE TNNLS 2022**. [[pdf]](https://ieeexplore.ieee.org/abstract/document/9722567/) Canjie Luo, <span style="color:PaleVioletRed;">Yuanzhi Zhu</span>, Lianwen Jin, Zhe Li, Dezhi Peng. 
- Learn to Augment: Joint Data Augmentation and Network Optimization for Text Recognition. **CVPR 2020**. [[pdf]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_Learn_to_Augment_Joint_Data_Augmentation_and_Network_Optimization_for_CVPR_2020_paper.pdf) Canjie Luo, <span style="color:PaleVioletRed;">Yuanzhi Zhu</span>, Lianwen Jin, Yongpan Wang.
- Decoupled Attention Network for Text Recognition. **AAAI 2020**. [[pdf]](https://ojs.aaai.org/index.php/AAAI/article/view/6903) Tianwei Wang, <span style="color:PaleVioletRed;">Yuanzhi Zhu</span>, Lianwen Jin, Canjie Luo, Xiaoxue Chen, Yaqiang Wu, Qianying Wang, Mingxiang Cai.
- Text Recognition in the Wild: A Survey. **ACM CSUR 2021**. [[pdf]](https://dl.acm.org/doi/abs/10.1145/3440756) Xiaoxue Chen, Lianwen Jin, <span style="color:PaleVioletRed;">Yuanzhi Zhu</span>, Canjie Luo, Tianwei Wang.
- Aggregation Cross-Entropy for Sequence Recognition. **CVPR 2019**. [[pdf]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Xie_Aggregation_Cross-Entropy_for_Sequence_Recognition_CVPR_2019_paper.pdf) Zecheng Xie, Yaoxiong Huang, <span style="color:PaleVioletRed;">Yuanzhi Zhu</span>, Lianwen Jin, Yuliang Liu, Lele Xie.


# ðŸŽ– Honors and Awards
- *2019.08* First place in three tasks of ICDAR 2019 Historical Document Reading Challenge on Large Structured Chinese Family Records. 
- *2019.05* First Place (1/1071) and the Best Algorithm Award in the Ancient Text Recognition Task at Digital China Innovation Competition.
- *2019.10* National Scholarship.
- *2016.10* National Endeavor Scholarship.
- *2015.10* National Endeavor Scholarship.

# ðŸ“– Educations
- *2018.09 - 2021.06*, Master of Engineering, South China University of Technology. Computer Vision, Optical Character Recognition.
- *2014.09 - 2018.06*, Bachelor of Engineering, South China University of Technology. Information Engineering (Elite Program).

# ðŸ’¬ Invited Talks
- *2023.08*, DocMaster: A Precise Document Parsing Agent Based on Large Language Models and OCR. [[Video]](https://www.bilibili.com/video/BV1BN411875v) [[Modelscope]](https://www.modelscope.cn/studios/iic/DocMaster)
